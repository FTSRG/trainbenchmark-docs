\chapter{Benchmark Results}
\label{cha:benchmarkresults}

\section{Benchmarking Environment}
\label{sec:environment}

For the implementation details, source codes and raw results, see the benchmark website\footnote{\url{https://incquery.net/publications/trainbenchmark/full-results}}. In this section we describe the runtime environment, and highlight some design decisions.

The benchmark machine contains two dual-core Intel Xeon (3.00~GHz) CPU, 12~GBs of RAM and an SAS disk formatted to ext4 for storing the models. In order to alleviate disturbance of a running measurement and minimize noise in the results, a bare metal 64-bit Ubuntu 12.10 OS was installed with unnecessary services (like \texttt{cron}) turned off. Oracle JVM version 1.7.0\_51 is used as the Java environment and Eclipse Kepler Modeling 64-bit for Linux to satisfy specific tool dependencies.

The performance measurements of a tool was independent from the others, i.e.\ for every tool only its codebase was loaded, and every measurement of a scenario was started in a different JVM. Before the execution, the OS file cache was cleared, and swapping was disabled to avoid this kind of thrashing. Each test case (including all phases) must be run within a specified time limit (15 minutes), otherwise its process was killed.

% TODO re-introduce memory measurements
% Before acquiring memory usage (free heap space) from the JVM, GC calls were triggered five times to sweep unfreed objects from the RAM. For a JVM, 6~GB heap limit was specified. Note that we are aware that this is a limited approach to memory usage measurements as it only takes ``retained'' heap consumption into account, and does not characterize the transient memory usage characteristics (i.e. allocations while the query evaluation is running). This shortcoming is planned to be addressed in future work.
%, and to compensate 64-bit pointers, OOPS (ordinary object pointers) compression was also turned on with the following options: (\code{-XX:MaxPermSize=256M -XX:+UseCompressedOops -Xmx25G}).

In the benchmark all cases were run 10 times, and the results were dumped into files, which were aggregated using the R statistical framework. The correlation results and performance plots are written into an HTML report.


\section{Tools}
\label{tools}
The measured tools generally work on graph-based models (like EMF~\cite{EMF} or RDF~\cite{RDF}), and provide a graph pattern-like query language. \autoref{tbl:tools} shows the list of currently integrated tools.

\begin{table}[h]
	\centering
	\footnotesize
	\begin{tabular}{  | l | r | l | m{1.4cm} | c | >{\centering}m{1.9cm} | m{2.3cm} | }
	\hline
	\multicolumn{1}{|c|}{\bf Tool} & 
	\multicolumn{1}{c|}{\bf Version} & 
	\multicolumn{1}{c|}{\bf Model DL} & 
	\bf Query language & 
	\multicolumn{1}{c|}{\bf Incremental} & 
	\bf In-memory only & 
	\bf Implementation language \\ \hline 
	Java & 7.0 & EMF & Java & \ding{109} & \ding{108} & C++ \\ \hline
	Eclipse OCL & 3.3.0 & EMF & OCL & \ding{109} & \ding{108} & Java \\ \hline
	%OCL Impact Analyzer & 3.1.0 & EMF & OCL & \ding{108} & \ding{108} & Java \\ \hline
	\eiq{} & 0.7.2 & EMF & IQPL & \ding{108} & \ding{108} & Java \\ \hline
	Drools & 5.4.0 & EMF & DRL & \ding{108} & \ding{108} & Java \\ \hline
	Sesame & 2.7.9 & RDF & SPARQL & \ding{109} & \ding{108} & Java \\ \hline
	4store & 1.1.5 & RDF & SPARQL & \ding{109} & \ding{109} & C \\ \hline
	Neo4j & 1.9.2 & Graph & Cypher & \ding{109} & \ding{109} & Java \\ \hline
	\end{tabular}
	\caption{Tools used in the benchmark.}
	\label{tbl:tools}
\end{table}

Note on \textbf{incrementality}: \emph{incremental} means that the tool not only employs caching techniques, but provides a dedicated incremental query evaluation algorithm that processes \emph{changes} in the model and propagates these changes to query evaluation results in an incremental way (i.e. avoiding complete recalculations). Both \textsc{EMF-IncQuery} and JBoss Drools are based on the Rete algorithm; OCL also has an incremental extension called the \emph{OCL Impact Analyzer} which is planned to be included in the measurements in the final version of the tech report.

In contrast, \emph{non-incremental} tools are using \emph{search-based} algorithms that evaluate each query with a model traversal, which may be optimized using heuristics and/or caching mechanisms. The essential difference to incremental tools is that they do not process changes in the model, but only take the current state of the model as the input.

Note on \textbf{in memory operations}: most of the integrated tools use models that are materialized into operating memory (RAM), hence query evaluation does not involve (expensive) disk operations. Some tools do not support in-memory backends, so to compensate for the disadvantage, such tools are used with files that are stored on RAM disks.

\subsection{EMF-based Tools}

\subsubsection{Java}
An imperative \emph{local search-based} approach was implemented in Java, operating on \emph{Eclipse Modeling Framework (EMF)}~\cite{EMF} models. Queries are implemented as Java functions, traversing a model without any search plan optimization, but they cut unnecessary search branches at the earliest possibility.

\subsubsection{Eclipse OCL}

The OCL~\cite{OCL} language is commonly used for querying EMF model instances in validation frameworks. It is a standardized navigation-based query language, applicable over a range of modeling formalisms. Taking advantage of the expressive features and wide-spread adoption of this query language, the project Eclipse OCL~\cite{EclipseOCL} provides a powerful query interface that evaluates such expressions over EMF models.

%\subsection{OCL Impact Analyzer}
%The Eclipse OCL project also supports incremental evaluation by including an Impact Analyzer (IA)~\cite{ocl-ia2} that calculates the constraints to be reevaluated based on a model change. During EMF modifications it looks for possible context objects that could change the match set, and re-evaluation can be executed only for those objects. As it is intended only for incremental use, basic Eclipse OCL is used for calculating the first result set (batch mode).

\subsubsection{\eiq{}}
\eiq{}~\cite{models10} is an Eclipse Modeling project that provides incremental query evaluation using Rete~\cite{rete} nets. Queries can be written in its graph pattern based query language (IncQuery Pattern Language, IQPL~\cite{iqpl}), which is evaluated over EMF models.

\subsubsection{Drools}
Incremental query evaluation is also supported by the \concept{Drools}~\cite{Drools} rule engine developed by Red Hat. It is based on a variant of Rete~\cite{rete} (object-oriented Rete). Queries can be formalized using its own rule description language. Queries can be constructed by naming the ''when'' part of rules and acquiring their matches. While Drools is not an EMF tool per se, the Drools implementation of the \tb{} works on EMF models.


\subsection{RDF-based Tools}

\subsubsection{Sesame}
Sesame gives an API specification for many tools, and also provides its own implementation. The tool evaluates queries over RDF that are formulated as SPARQL~\cite{Sparql} graph patterns.

\subsubsection{4store}
4store~\cite{harris20094store} is an open source, distributed triplestore implemented in C. The main goal of 4store is to provide a high performance storage and query engine for semantic web applications. 

\subsubsection{\iqd}
\iqd{}~\cite{Izso:2013:IIG:2487766.2487772} is a distributed incremental graph query engine developed in the Budapest University of Technology and Economics. The goal of \iqd{} is to provide a scalable query engine by using a cluster of servers instead of a single workstation.


\subsection{Graph-based Tools}

\subsubsection{Neo4j}
As part of the NoSQL movement, database management systems emerged with a focus on graph storage and processing. As of 2014, the most popular graph database is 
Neo4j~\cite{neo4j}. The data model is based on graphs, where any node or edge can be labeled. Cypher can be used to query labeled graphs using its own graph pattern notation. This engine also uses disk for data storing, so a RAM disk is created during the benchmark.

\subsection{SQL-based Tools}

\subsubsection{MySQL}
MySQL~\cite{mysql} is one of the most well-known and widely used open-source relational database management systems.


\section{Measurement Results for Performance Comparison}
\label{sec:results}

The measurement results of the benchmark are displayed below. These diagrams show the batch query performance and incremental evaluation time of each tool, for different model sizes. Additionally, the initial and the updated result set size is displayed under the model sizes for the batch and incremental queries, respectively.

\subsection{How to read the charts}
% log scales
% low-order polynomial characteristics: slope is the order
% ``constant'' difference: constant factor (order of magnitude) difference
% incomplete series: timeout

The charts are presented with logarithmic scales on both axes. This means that the general ``linear'' appearance of all measurement series correspond to a low-order polynomial big-O characteristic where the slope of the plot determines the dominant order (exponent). Moreover, a constant difference on these plots corresponds to a constant order-of-magnitude (i.e. constant multiplier) difference. As the execution runs with a timeout, incomplete measurement series indicate that the time limit has been reached during execution.
 
% phases
% batch validation = R+C0
% total revalidation = sum ExCx
% total time = everything together
% memory usage = retained heap after execution finished
% series of edit times = how Ex times vary depending on iteration number

The execution phases as described in \autoref{sec:phases} have been grouped into several aggregated results as follows:

\begin{itemize}
  \item \emph{Batch validation} (\autoref{sec:results:batchvalidation}) corresponds to the sum of \emph{read} and \emph{check} execution times, as it models a batch validation scenario where the model is read and validated.
  \item \emph{Revalidation} (\autoref{sec:results:revalidation}) corresponds to the sum of \emph{edit}-\emph{recheck} times as it represents the cumulative execution time of model editing-revalidation cycles.
  \item \emph{Total time} (\autoref{sec:results:totaltime}) represents the combined execution time of the entire measurement cycle, giving an overall performance indicator.
  \item \emph{Series of edit times} (\autoref{sec:results:seriesedit}) presents a deeper analysis to show how long each \emph{edit-re-check} cycle takes. These plots are useful for observing transient effects such as JIT HotSpot compilation.
  \item \emph{Series of revalidation times} (\autoref{sec:results:seriesrevalidation}) present the same for the \emph{re-check} phase.
\end{itemize}

% observe variations with increasing query complexity (PL-RS-SS-SN)

Each plot shows execution times corresponding to a combination of one of the queries of \autoref{sec:queries} and a tranformation scenario as defined in \autoref{sec:transformatios}. The plots offer insights into performance characteristics in three major ways:

\begin{itemize}
  \item \emph{Execution time vs. instance model size.} Each plot can be directly interpreted as an evaluation of performance against increasing model size, where different tool characteristics can be compared in a single plot.
  \item \emph{Characteristics vs. increasing query complexity.} As the queries are of different complexity, comparing the plots within the same transformation scenario group but against different queries is useful to judge tool characteristics against increasing query complexity (or query result size, shown as ``Results'' on the Y axis).
  \item \emph{Characteristics vs. increasing transformation complexity.} As the transformation scenarios are characteristically different (e.g. with respect to the amount of objects modified by the transformation sequence -- as shown on the X axis as ``Modifications''), comparing the plots with the same query but against different scenarions is useful to judge tool characteristics against various transformation complexities.
\end{itemize}



\subsection{Batch Validation}
\label{sec:results:batchvalidation}
\subsubsection{Batch Validation (User Scenario)}
\forallqueries{\benchmarkresult{User_ReadPCheck0_\n}{Batch validation times for the \textsf{\n} query in the \textsf{User} scenario.}}
\subsubsection{Batch Validation (XForm Scenario)}
\forallqueries{\benchmarkresult{XForm_ReadPCheck0_\n}{Batch validation times for the \textsf{\n} query in the \textsf{XForm} scenario.}}

\subsection{Revalidation}
\label{sec:results:revalidation}
\subsubsection{Revalidation (User Scenario)}
\forallqueries{\benchmarkresult{User_SumModifyPSumReCheck_\n}{Revalidation times for the \textsf{\n} query in the \textsc{User} scenario.}}
\subsubsection{Revalidation (XForm Scenario)}
\forallqueries{\benchmarkresult{XForm_SumModifyPSumReCheck_\n}{Revalidation times for the \textsf{\n} query in the \textsf{XForm} scenario.}}

\subsection{Total time}
\label{sec:results:totaltime}
\subsubsection{Total Time (User Scenario)}
\forallqueries{\benchmarkresult{User_SumTime_\n}{Total time for the \textsf{\n} query in the \textsf{User} scenario.}}
\subsubsection{Total Time (XForm Scenario)}
\forallqueries{\benchmarkresult{XForm_SumTime_\n}{Total time for the \textsf{\n} query in the \textsf{XForm} scenario.}}

% \subsection{Memory usage}
% \label{sec:results:memoryusage}
% \subsubsection{Memory Usage (User Scenario)}
% \forallqueries{\benchmarkresult{User_Memory_\n}{Memory consumption of the \textsf{\n} query in the \textsf{User} scenario.}}
% \subsubsection{Memory Usage (XForm Scenario)}
% \forallqueries{\benchmarkresult{User_Memory_\n}{Memory consumption of the \textsf{\n} query in the \textsf{XForm} scenario.}}

\subsection{Series of Edit times}
\label{sec:results:seriesedit}
\subsubsection{Series of Edit Times (User Scenario)}
\benchmarkresult{series_DetEdit_64_PosLength}{Series of edit times for the \textsf{PosLength} query in the \textsf{User} scenario.}
\benchmarkresult{series_DetEdit_16_RouteSensor}{Series of edit times for the \textsf{RouteSensor} query in the \textsf{User} scenario.}
\benchmarkresult{series_DetEdit_1_SignalNeighbor}{Series of edit times for the \textsf{SignalNeighbor} query in the \textsf{User} scenario.}
\benchmarkresult{series_DetEdit_128_SwitchSensor}{Series of edit times for the \textsf{SwitchSensor} query in the \textsf{User} scenario.}

\subsection{Series of Incremental Revaliation Times}
\label{sec:results:seriesrevalidation}
\subsubsection{Series of Incremental Revalidation Times (User Scenario)}
\benchmarkresult{series_DetCheck_64_PosLength}{Series of incremental revalidation times for the \textsf{PosLength} query in the \textsf{User} scenario.}
\benchmarkresult{series_DetCheck_16_RouteSensor}{Series of incremental revalidation times for the \textsf{RouteSensor} query in the \textsf{User} scenario.}
\benchmarkresult{series_DetCheck_1_SignalNeighbor}{Series of incremental revalidation times for the \textsf{SignalNeighbor} query in the \textsf{User} scenario.}
\benchmarkresult{series_DetCheck_128_SwitchSensor}{Series of incremental revalidation times for the \textsf{SwitchSensor} query in the \textsf{User} scenario.}
